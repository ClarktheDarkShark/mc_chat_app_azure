import os
import time
import cv2
import openai
import pyautogui
import pytesseract
import numpy as np
from PIL import Image

# ----------------------------
# Configuration
# ----------------------------

# Set your OpenAI API key here or use the environment variable.
openai.api_key = os.getenv("OPENAI_API_KEY", "your-openai-api-key")

# Optionally, set the path for Tesseract OCR if it's not in your PATH.
# For example:
# pytesseract.pytesseract.tesseract_cmd = r'/usr/local/bin/tesseract'

# ----------------------------
# Module: Screen Capture
# ----------------------------

class ScreenCapture:
    @staticmethod
    def capture(region=None):
        """
        Captures a screenshot.
        :param region: Optional tuple (left, top, width, height)
        :return: PIL Image object of the screenshot.
        """
        screenshot = pyautogui.screenshot(region=region)
        return screenshot

    @staticmethod
    def save_image(image, filename):
        """
        Saves a PIL image to a file.
        :param image: PIL Image object.
        :param filename: Path to save the image.
        """
        image.save(filename)
        print(f"[ScreenCapture] Image saved: {filename}")

# ----------------------------
# Module: OCR Processing
# ----------------------------

class OCRProcessor:
    @staticmethod
    def process_image(pil_image):
        """
        Extract text from a PIL Image using Tesseract OCR.
        :param pil_image: PIL Image object.
        :return: Extracted text string.
        """
        text = pytesseract.image_to_string(pil_image)
        return text

# ----------------------------
# Module: UI Element Detection
# ----------------------------

class UIDetector:
    @staticmethod
    def match_template(screen_image, template_path, threshold=0.8):
        """
        Uses template matching to find a UI element.
        :param screen_image: PIL Image of the current screen.
        :param template_path: Path to the template image file.
        :param threshold: Matching threshold between 0 and 1.
        :return: Coordinates (x, y) of top-left corner of best match if above threshold, else None.
        """
        # Convert PIL image to OpenCV image
        screen_cv = cv2.cvtColor(np.array(screen_image), cv2.COLOR_RGB2BGR)
        template = cv2.imread(template_path, cv2.IMREAD_COLOR)
        if template is None:
            print(f"[UIDetector] Error: Template image not found at {template_path}")
            return None

        result = cv2.matchTemplate(screen_cv, template, cv2.TM_CCOEFF_NORMED)
        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)

        if max_val >= threshold:
            print(f"[UIDetector] Template matched with confidence: {max_val}")
            return max_loc  # (x, y) coordinates
        else:
            print(f"[UIDetector] No match found above threshold {threshold}. Best confidence: {max_val}")
            return None

# ----------------------------
# Module: UI Controller
# ----------------------------

class UIController:
    @staticmethod
    def move_and_click(x, y, delay=0.5):
        """
        Moves the mouse to (x, y) and performs a click.
        :param x: X-coordinate.
        :param y: Y-coordinate.
        :param delay: Delay after clicking, in seconds.
        """
        print(f"[UIController] Moving cursor to: ({x}, {y}) and clicking.")
        pyautogui.moveTo(x, y)
        pyautogui.click()
        time.sleep(delay)

    @staticmethod
    def type_text(text, delay=0.1):
        """
        Types text into the currently focused field.
        :param text: Text to type.
        :param delay: Delay between keystrokes.
        """
        print(f"[UIController] Typing text: {text}")
        pyautogui.write(text, interval=delay)

# ----------------------------
# Module: LLM Controller (OpenAI API Integration)
# ----------------------------

class LLMController:
    def __init__(self, model="gpt-3.5-turbo"):
        self.model = model

    def get_response(self, prompt, system_message=None, temperature=0.7):
        """
        Sends a prompt to the OpenAI API and returns the response.
        :param prompt: The prompt to send.
        :param system_message: Optional system message to define behavior.
        :param temperature: Sampling temperature.
        :return: The text response from the model.
        """
        messages = []
        if system_message:
            messages.append({"role": "system", "content": system_message})
        messages.append({"role": "user", "content": prompt})

        try:
            response = openai.ChatCompletion.create(
                model=self.model,
                messages=messages,
                temperature=temperature
            )
            reply = response["choices"][0]["message"]["content"]
            print(f"[LLMController] LLM Response: {reply}")
            return reply
        except Exception as e:
            print("[LLMController] Error communicating with OpenAI:", e)
            return ""

# ----------------------------
# Main Application Logic
# ----------------------------

def main():
    # Initialize controllers
    screen_capturer = ScreenCapture()
    ocr_processor = OCRProcessor()
    ui_detector = UIDetector()
    ui_controller = UIController()
    llm_controller = LLMController()

    # Specify the template image for UI element detection (update with your own image file)
    template_image_path = "button.png"

    print("Starting interactive UI automation. Type 'exit' at any prompt to end.")

    while True:
        print("\n--- New Cycle ---")
        # 1. Capture the screen.
        screenshot = screen_capturer.capture()
        screen_capturer.save_image(screenshot, "screenshot.png")

        # 2. Process OCR on the screenshot.
        text_on_screen = ocr_processor.process_image(screenshot)
        print("[Main] Extracted text from screen:")
        print(text_on_screen)

        # 3. Detect a UI element using template matching.
        match_location = ui_detector.match_template(screenshot, template_image_path)
        if match_location:
            x, y = match_location
            detected_action = f"Click at location ({x}, {y})"
        else:
            detected_action = "No clickable UI element detected"
        print(f"[Main] Detected action: {detected_action}")

        # 4. Use LLM (OpenAI API) to get a suggestion about what to do next.
        prompt = f"I have scanned the screen and obtained the following text:\n{text_on_screen}\n" \
                 f"And detected the following UI element: {detected_action}.\n" \
                 f"Based on this, what should I do next? (e.g., 'click the button', 'fill text', or 'do nothing')"
        system_message = "You are a helpful assistant that analyzes UI screens and suggests automated actions."
        llm_suggestion = llm_controller.get_response(prompt, system_message=system_message)

        # 5. Ask the user if they would like to proceed with the suggested action.
        print("\n[User Interaction] Suggestion: " + llm_suggestion)
        user_input = input("Would you like to proceed with this action? (yes/no/exit): ").strip().lower()
        if user_input == "exit":
            print("Exiting interactive automation loop.")
            break

        if user_input == "yes":
            # Parse the LLM suggestion for an action.
            if "click" in llm_suggestion.lower() and match_location:
                ui_controller.move_and_click(x, y)
            elif "fill" in llm_suggestion.lower():
                # Here, you can prompt the user for the text to fill or use a default.
                text_to_fill = input("Enter the text you would like to fill: ")
                if text_to_fill.lower() == "exit":
                    print("Exiting automation loop.")
                    break
                ui_controller.type_text(text_to_fill)
            else:
                print("[Main] No specific actionable command recognized, or UI element not detected.")
        else:
            print("[Main] User opted not to proceed with the suggested action.")

        # 6. Ask if the user wants to continue with further cycles.
        continue_input = input("Would you like to perform another cycle? (yes/exit): ").strip().lower()
        if continue_input != "yes":
            print("Exiting the automation loop.")
            break

        # Optional: Delay before the next cycle
        time.sleep(1)

if __name__ == "__main__":
    main()
